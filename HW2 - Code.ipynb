{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3b2fba38",
      "metadata": {
        "id": "3b2fba38"
      },
      "source": [
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4b5223",
      "metadata": {
        "id": "0c4b5223"
      },
      "source": [
        "## Due: 11:59PM PST, Nov 10, 2023\n",
        "\n",
        "In this assignment, we will implement (1) depthwise convolution, (2) group convolution, (3) softmax, (4) scaled dot product attention module, and (4) asymmetric linear quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76f8a6f",
      "metadata": {
        "id": "a76f8a6f"
      },
      "source": [
        "## Helper code: Please do not modify this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "29d6d5a1",
      "metadata": {
        "id": "29d6d5a1"
      },
      "outputs": [],
      "source": [
        "# Please do not modify this cell\n",
        "import torch\n",
        "import torch.quantization\n",
        "\n",
        "from typing import Final, Tuple\n",
        "import random\n",
        "import math\n",
        "\n",
        "filter_res = [1, 3, 5]\n",
        "channel_count = [8, 16]\n",
        "num_groups_list = [1, 4, 8]\n",
        "\n",
        "\n",
        "number_of_tests: Final[int] = 10\n",
        "\n",
        "def test_res_printer(\n",
        "    test_res: Tuple[int],\n",
        "    test_name: str = \"Test\"\n",
        ") -> None:\n",
        "    print(\"<Test {} Results>\".format(test_name))\n",
        "    print(\"- Correct: {}\".format(test_res[0]))\n",
        "    print(\"- Incorrect: {}\".format(test_res[1]))\n",
        "    print(\"-> Passed {}% of tests\".format((test_res[0]/test_res[2])*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb28a228",
      "metadata": {
        "id": "bb28a228"
      },
      "source": [
        "## Group convolution example\n",
        "\n",
        "Group convolution and its special case, depth-wise convolution, are widely used. However, not many materials describe how they work in detail. We already covered the basics in the lecture at a high level. However, to deeply understand how it works, taking a look at code examples will be helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "5d6f5225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6f5225",
        "outputId": "26564610-2d14-4133-d488-d29e6a40397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Group convolution case>\n",
            "Input shape: torch.Size([100, 10, 10])\n",
            "Weight shape: torch.Size([10, 20, 3, 3])\n",
            "Output shape: torch.Size([10, 8, 8])\n",
            "\n",
            "<Normal Conv2D case>\n",
            "Input shape: torch.Size([100, 10, 10])\n",
            "Weight shape: torch.Size([10, 100, 3, 3])\n",
            "Output shape: torch.Size([10, 8, 8])\n",
            "\n",
            ">> Group Conv2D generates the same-sized output with 80.0% less number of weights\n",
            "Please feel free to try this example again modifyng the example tensor dimensions and num_groups above\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "# Example Tensor dimensions\n",
        "input_channels = 100\n",
        "output_channels = 10\n",
        "input_height = 10\n",
        "input_width = 10\n",
        "filter_height = 3\n",
        "filter_width = 3\n",
        "\n",
        "# The number of groups must evenly divide num_input_channels and num_output_channels\n",
        "# This is a valid group size as 100 % 5 == 0, 10 % 5 == 0\n",
        "num_groups = 5\n",
        "\n",
        "##############################\n",
        "\n",
        "\n",
        "\n",
        "example_group_conv = torch.nn.Conv2d(\n",
        "    input_channels, # 100\n",
        "    output_channels, # 10\n",
        "    [filter_height, filter_width], #[3,3]\n",
        "    groups = num_groups # 5\n",
        ")\n",
        "\n",
        "example_input = torch.rand([input_channels,input_height,input_width]) # [N=1, C=100, H=10, W=10]\n",
        "groupd_conv2d_output = example_group_conv(example_input)\n",
        "\n",
        "print(\"<Group convolution case>\")\n",
        "print(\"Input shape: {}\".format(example_input.shape))\n",
        "print(\"Weight shape: {}\".format(example_group_conv.weight.shape))\n",
        "print(\"Output shape: {}\\n\".format(groupd_conv2d_output.shape))\n",
        "\n",
        "# Now compare with typical CONV2D without any groups\n",
        "# The only difference is that we will use groups = 1\n",
        "example_conv2d = torch.nn.Conv2d(\n",
        "    input_channels, # 100\n",
        "    output_channels, # 10\n",
        "    [filter_height, filter_width], #[3,3]\n",
        "    groups=1 # no splitting into groups\n",
        ")\n",
        "\n",
        "conv2d_output = example_conv2d(example_input)\n",
        "\n",
        "print(\"<Normal Conv2D case>\")\n",
        "print(\"Input shape: {}\".format(example_input.shape))\n",
        "print(\"Weight shape: {}\".format(example_conv2d.weight.shape))\n",
        "print(\"Output shape: {}\\n\".format(conv2d_output.shape))\n",
        "\n",
        "num_weights_group_conv2d = example_group_conv.weight.numel()\n",
        "num_weights_conv2d = example_conv2d.weight.numel()\n",
        "\n",
        "print(\">> Group Conv2D generates the same-sized output with {}% less number of weights\".format(\n",
        "    (1 - num_weights_group_conv2d / num_weights_conv2d) * 100)\n",
        ")\n",
        "\n",
        "print(\"Please feel free to try this example again modifyng the example tensor dimensions and num_groups above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf317147",
      "metadata": {
        "id": "bf317147"
      },
      "source": [
        "## Problem 1 (20 pts)\n",
        "\n",
        "Please implement depthwise convolution. For simplicity, we won't use any padding and stride in this example (i.e., padding = [0, 0], stride = [1, 1]). Your implementation should match the behavior of \"torch.nn.Conv2d\" in Pytorch.\n",
        "(reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80d21716",
      "metadata": {
        "id": "80d21716"
      },
      "source": [
        "### Hints\n",
        "\n",
        "We will use the same dimension convention (notations) used in our lecture slides (N: batch, K: output channel, C: input channel, P/Q: Output height/width, H/W: input height/width, R/S: filter height/width). The depthwise convolution can be described as follows:\n",
        "\n",
        "$$\n",
        "  O[n][k][p][q] = \\sum_{r=0}^{R}\\sum_{s=0}^{S} W[k][0][r][s] \\times I[n][k][p+r][q+s]\n",
        "$$\n",
        "\n",
        "The tensor shapes are as follows:\n",
        "  - Weight: [K, 1, R, S]\n",
        "  - Input: [N, K, H, W]\n",
        "  - Output: [N, K, P, Q]\n",
        "\n",
        "Please take a detailed look at the weight shape and the depthwise convolution formula in this cell; this will provide you with a good amount of information about the operation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6abfd0e9",
      "metadata": {
        "id": "6abfd0e9"
      },
      "source": [
        "### Your Implementation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "c50f2357",
      "metadata": {
        "id": "c50f2357"
      },
      "outputs": [],
      "source": [
        "# Implement this cell\n",
        "def my_depthwise_conv2d(\n",
        "    weight: torch.Tensor, # [K, 1, R, S];\n",
        "    input_activation: torch.Tensor, # [1, C, H, W]\n",
        ") -> torch.Tensor: # [1, K, P, Q]\n",
        "    # raise NotImplementedError\n",
        "    K, _, R, S = weight.shape\n",
        "    N, C, H, W = input_activation.shape\n",
        "    P = H - R + 1\n",
        "    Q = W - S + 1\n",
        "\n",
        "    output = torch.zeros([N, K, P, Q])\n",
        "\n",
        "    for n in range(N):\n",
        "        for c in range(C):\n",
        "            for k in range(K//C):\n",
        "                for p in range(P):\n",
        "                    for q in range(Q):\n",
        "                        output[n, c*(K//C)+k, p, q] = torch.sum(weight[c*(C//K)+k, ...] * input_activation[n, c, p:p+R, q:q+S])\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47670e1b",
      "metadata": {
        "id": "47670e1b"
      },
      "source": [
        "### Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "257631af",
      "metadata": {
        "id": "257631af"
      },
      "outputs": [],
      "source": [
        "# Test program: Please do not modify this cell\n",
        "def test_depthwise_conv2d(num_tests = number_of_tests):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "\n",
        "    for test_id in range(num_tests):\n",
        "        activaiton_resolution = torch.randint(7, 32,[2,])\n",
        "\n",
        "        K = int(random.choice(channel_count))\n",
        "        C = K\n",
        "        H = activaiton_resolution[0]\n",
        "        W = activaiton_resolution[1]\n",
        "        R = int(random.choice(filter_res))\n",
        "        S = R\n",
        "        G = C\n",
        "\n",
        "        if H < R:\n",
        "            H += R\n",
        "\n",
        "        if W < S:\n",
        "            W += S\n",
        "\n",
        "        # Generate random input activation and weight\n",
        "        weight = torch.randn(K, 1 , R, S)\n",
        "\n",
        "        input_activation = torch.randn(1,C,H,W)\n",
        "\n",
        "        # Compute the reference output\n",
        "        reference_conv = torch.nn.Conv2d(C, K, (R,S), groups = G, bias=False)\n",
        "        reference_conv.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "        reference_output = reference_conv(input_activation)\n",
        "        # print(f'refout in shape {reference_output.shape}')\n",
        "\n",
        "        # Compute the test output\n",
        "        test_output = my_depthwise_conv2d(weight, input_activation)\n",
        "        # print(f'myout in shape {test_output.shape}')\n",
        "\n",
        "        if torch.allclose(reference_output, test_output,  atol=1e-3):\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "        print(\"Finished test run {}\".format(test_id))\n",
        "\n",
        "    return (correct, incorrect, num_tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "c6d44598",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6d44598",
        "outputId": "0f4922b4-9f56-4cd7-c7e2-f0f6031c8f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished test run 0\n",
            "Finished test run 1\n",
            "Finished test run 2\n",
            "Finished test run 3\n",
            "Finished test run 4\n",
            "<Test Problem 1 Results>\n",
            "- Correct: 5\n",
            "- Incorrect: 0\n",
            "-> Passed 100.0% of tests\n"
          ]
        }
      ],
      "source": [
        "# You can run tests by running this cell\n",
        "\n",
        "test_res = test_depthwise_conv2d(5)\n",
        "test_res_printer(test_res, \"Problem 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e584c6",
      "metadata": {
        "id": "d9e584c6"
      },
      "source": [
        "## Problem 2 (50 pts)\n",
        "\n",
        "Please implement the group convolution, which is a generalization of the depth-wise convolution. The behavior of your function should match that of \"torch.nn.Conv2d\" in Pytorch.\n",
        "\n",
        "(reference: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ce78b1",
      "metadata": {
        "id": "b5ce78b1"
      },
      "source": [
        "### Hint\n",
        "\n",
        "\n",
        "#### High-level overview\n",
        "Group convolution can be seen as parallel convolutions, splitted from a regular CONV2D. When then number of groups = G, we split input and output channels into G groups. Each group contains C/G input channels and K/G output channels, which means C and K must be dividable by G.\n",
        "\n",
        "For example, if K = 20, C = 10, and G = 2, we have two independent CONV2Ds that have K = 10 and C = 5 for each. After computing the individual CONV2Ds, we concatenate the results in output channel dimension.\n",
        "\n",
        "#### More detailed example\n",
        "\n",
        "Let's take a look at another more concrete example:\n",
        "\n",
        "We want to split a CONV2D with dimensions N = 1, K = 4, C = 10, H = 3, W = 3, R = 1, and S = 1, into two groups. Let's name the two sub_conv2D ops as sub_conv2D_0 and sub_conv2D_1, respectively. Also, let's use conv2d_full for the original conv2d before splitting.\n",
        "\n",
        "\n",
        "\n",
        "Then, inputs are splitted as follows:\n",
        "$$\n",
        "Input_{sub\\_conv2D\\_0} = Input_{conv2d\\_full}[n=0][\\color{red}{c=0,1,2,3,4}][h=0,1,2][w=0,1,2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Input_{sub\\_conv2D\\_1} = Input_{conv2d\\_full}[n=0][\\color{red}{c=5,6,7,8,9}][h=0,1,2][w=0,1,2]\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "For weights, we are not simply splitting them into two. Instead, we define a new weight tensor, $New\\_Weight$, reflecting the reduced amount of weights for the group convolution arithmetic. The shape of $New\\_Weight$ is [K/G, C/G, R, S], which is [4/2, 10/2, 1, 1] == [2, 5, 1, 1] in this example.\n",
        "\n",
        "$$\n",
        "Weight_{sub\\_conv2D\\_0} = New\\_Weight[\\color{blue}{k=0,1}][\\color{red}{c'=0,1,2,3,4}][r=0][s=0]\n",
        "$$\n",
        "\n",
        "$$\n",
        "Weight_{sub\\_conv2D\\_1} = New\\_Weight[\\color{blue}{k=2,3}][\\color{red}{c'=0,1,2,3,4}][r=0][s=0]\n",
        "$$\n",
        "\n",
        "Note that the input channel dimension of weights is denoted as c', not c. The size of c' dimension is C/G where C is the input channel count of conf2d_full, and G is the number of groups.\n",
        "\n",
        "\n",
        "\n",
        "With these new inputs and weights, we compute independent convolutions and concatenate the results into the output channel of the output activation.\n",
        "\n",
        "$$\n",
        "Output_{group\\_conv}[n=0][\\color{blue}{k=0,1}][p=0,1,2][q=0,1,2] = Output_{sub\\_conv2D\\_0}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Output_{group\\_conv}[n=0][\\color{blue}{k=2,3}][p=0,1,2][q=0,1,2] = Output_{sub\\_conv2D\\_1}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27e9eb2",
      "metadata": {
        "id": "e27e9eb2"
      },
      "source": [
        "### Your Implementation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "2ccef5d2",
      "metadata": {
        "id": "2ccef5d2"
      },
      "outputs": [],
      "source": [
        "# Note: Please read the tensor dimension in the comment carefully\n",
        "def my_group_conv2d(\n",
        "    weight: torch.Tensor, # [K, C/G, R, S]\n",
        "    input_activation: torch.Tensor, # [1, C, H, W]\n",
        "    num_groups: int,\n",
        ") -> torch.Tensor: # [1, K, P, Q]\n",
        "\n",
        "    # raise NotImplementedError\n",
        "    K, C_, R, S = weight.shape\n",
        "    N, C, H, W = input_activation.shape\n",
        "    G = num_groups\n",
        "    assert C_ == C/G\n",
        "    assert C % G == 0\n",
        "    assert K % G == 0\n",
        "    P = H - R + 1\n",
        "    Q = W - S + 1\n",
        "\n",
        "    output = torch.zeros([N, K, P, Q])\n",
        "    # print(input_activation.shape)\n",
        "    # print(weight.shape)\n",
        "    # print(G)\n",
        "    # print(output.shape)\n",
        "\n",
        "    for g in range(G):\n",
        "        for n in range(N):\n",
        "            for k in range(K//G):\n",
        "                for c in range(C//G):\n",
        "                    for p in range(P):\n",
        "                        for q in range(Q):\n",
        "                            output[n, g*(K//G)+k, p, q] += torch.sum(input_activation[n, g*(C//G)+c, p:p+R, q:q+S] * weight[g*(K//G)+k, c, ...])\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709ae9fb",
      "metadata": {
        "id": "709ae9fb"
      },
      "source": [
        "### Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "664a2e02",
      "metadata": {
        "id": "664a2e02"
      },
      "outputs": [],
      "source": [
        "# Test program: Please do not modify this cell\n",
        "def test_group_conv2d(num_tests = number_of_tests):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "\n",
        "    for test_id in range(num_tests):\n",
        "        activaiton_resolution = torch.randint(7, 32,[2,])\n",
        "\n",
        "        K = int(random.choice(channel_count))\n",
        "        C = int(random.choice(channel_count))\n",
        "        H = activaiton_resolution[0]\n",
        "        W = activaiton_resolution[1]\n",
        "        R = int(random.choice(filter_res))\n",
        "        S = R\n",
        "        G = int(random.choice(num_groups_list))\n",
        "\n",
        "        if H < R:\n",
        "            H += R\n",
        "\n",
        "        if W < S:\n",
        "            W += S\n",
        "\n",
        "        # Generate random input activation and weight\n",
        "        weight = torch.randn(K,int(C/G),R,S)\n",
        "\n",
        "        input_activation = torch.randn(1,C,H,W)\n",
        "\n",
        "        # Compute the reference output\n",
        "        reference_conv = torch.nn.Conv2d(C, K, (R,S), groups = G, bias=False)\n",
        "        reference_conv.weight = torch.nn.Parameter(weight)\n",
        "\n",
        "        reference_output = reference_conv(input_activation)\n",
        "\n",
        "        # Compute the test output\n",
        "        test_output = my_group_conv2d(weight, input_activation, G)\n",
        "\n",
        "        if torch.allclose(reference_output, test_output,  atol=1e-3):\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "        print(\"Finished test run {}\".format(test_id))\n",
        "\n",
        "    return (correct, incorrect, num_tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "40322487",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40322487",
        "outputId": "dd6ab46d-fe4b-42ae-c6ad-3208d6997825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished test run 0\n",
            "Finished test run 1\n",
            "Finished test run 2\n",
            "Finished test run 3\n",
            "Finished test run 4\n",
            "<Test Problem 2 Results>\n",
            "- Correct: 5\n",
            "- Incorrect: 0\n",
            "-> Passed 100.0% of tests\n"
          ]
        }
      ],
      "source": [
        "# You can run this test to validate your implementation\n",
        "test_res = test_group_conv2d(5)\n",
        "test_res_printer(test_res, \"Problem 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf0718c",
      "metadata": {
        "id": "3bf0718c"
      },
      "source": [
        "## Problem 3 (50 pts)\n",
        "\n",
        "In problems 3, 4, and 5, we will implement building blocks for a self-attention module in Transformer models and combine them to implement our own scaled dot product attention module. Like previous problems, you are expected to implement your own version of the module and the generate results will be compared against a reference implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb868b42",
      "metadata": {
        "id": "bb868b42"
      },
      "source": [
        "In problem 3, you will implement your own version of softmax. Please read the requirement and tips carefully and implement your softmax.\n",
        "\n",
        "### Requirements\n",
        "- You cannot use pre-built softmax functions (e.g., torch.nn.softmax)\n",
        "- You can use Pytorch functions that do not directly implement the softmax (e.g., torch.exp)\n",
        "\n",
        "### Spec\n",
        "- The softmax function expects a 3D tensor as its input and outputs a 3D tensor that has the same shape as the input tensor\n",
        "- The data dimension of the input tensor is [B, L, L], where B and L refer to the batch and sequence length, respetively. (The dimensionality is based on the Transformer use case.)\n",
        "- Your softmax should calculate softmax on the last dimension (L), like we do in the Transformer model. To clarify, I mean the right-most L dimension in the shape definition ([B, L, L]).\n",
        "\n",
        "### Hint\n",
        "\n",
        "- You should calcuate softmax on each row of the given tensor (i.e., for each batch, for each row, compute softmax across columns within the row).\n",
        "- If you directly implement softmax following the mathmatical definition, you will likely encounter some \"issues.\" Figuring out a solution is the discussion problem; I will not directly let you know about the solution. Please utilize all the resources and tools available to find a solution (imagine you encoutered such a situation in your future job; you must address the issue)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec85079",
      "metadata": {
        "id": "6ec85079"
      },
      "source": [
        "### Your Implementation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "66e18df3",
      "metadata": {
        "id": "66e18df3"
      },
      "outputs": [],
      "source": [
        "# Implement this cell\n",
        "\n",
        "# B: Batch size\n",
        "# L: Sequence length\n",
        "# D: Embedding dimension\n",
        "\n",
        "def my_softmax(\n",
        "    input_tensor: torch.Tensor, # Tensor: [B, L, D]\n",
        ") -> torch.Tensor:\n",
        "    temp = input_tensor - torch.max(input_tensor, dim=-1, keepdim=True).values\n",
        "    temp = torch.exp(temp)\n",
        "    return temp / torch.sum(temp, axis=-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a39e70",
      "metadata": {
        "id": "44a39e70"
      },
      "source": [
        "### Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "89450317",
      "metadata": {
        "id": "89450317"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Test program: Please do not modify this cell\n",
        "def test_softmax(num_tests = number_of_tests):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "\n",
        "    for test_id in range(num_tests):\n",
        "        batch_size = torch.randint(1,8,[1,])\n",
        "        tensor_dim = torch.randint(1,1024, [1,])\n",
        "\n",
        "        # Generate a random tensor\n",
        "        softmax_input = torch.rand(batch_size, tensor_dim[0], tensor_dim[0]) * 256\n",
        "\n",
        "        # Compute the reference output\n",
        "        reference_softmax = torch.nn.Softmax(dim=2)\n",
        "        reference_output = reference_softmax(softmax_input)\n",
        "        # Compute the test output\n",
        "        test_output = my_softmax(softmax_input)\n",
        "\n",
        "        #print(reference_output)\n",
        "\n",
        "        if torch.allclose(reference_output, test_output,  atol=1e-3):\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "        print(\"Finished test run {}\".format(test_id))\n",
        "\n",
        "    return (correct, incorrect, num_tests)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "24c32b19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c32b19",
        "outputId": "a165f15d-e492-4fbb-80c8-f03a10ab506b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished test run 0\n",
            "Finished test run 1\n",
            "Finished test run 2\n",
            "Finished test run 3\n",
            "Finished test run 4\n",
            "Finished test run 5\n",
            "Finished test run 6\n",
            "Finished test run 7\n",
            "Finished test run 8\n",
            "Finished test run 9\n",
            "Finished test run 10\n",
            "Finished test run 11\n",
            "Finished test run 12\n",
            "Finished test run 13\n",
            "Finished test run 14\n",
            "Finished test run 15\n",
            "Finished test run 16\n",
            "Finished test run 17\n",
            "Finished test run 18\n",
            "Finished test run 19\n",
            "<Test Problem 3 Results>\n",
            "- Correct: 20\n",
            "- Incorrect: 0\n",
            "-> Passed 100.0% of tests\n"
          ]
        }
      ],
      "source": [
        "test_res = test_softmax(20)\n",
        "test_res_printer(test_res, \"Problem 3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b36fd4f",
      "metadata": {
        "id": "3b36fd4f"
      },
      "source": [
        "## Problem 4 (30 pts)\n",
        "\n",
        "In problem 4, you will implement the multi-head scaled dot product attention block in the Transformer model.\n",
        "\n",
        "### Requirements\n",
        "- You cannot use pre-built Pytorch classes and functions that directly implement the scaled dot product attention block\n",
        "- You can use other Pytorch functions (e.g., torch.bmm)\n",
        "- You are implementing a block for inferences; no infrastructure for training is required\n",
        "\n",
        "### Spec\n",
        "- Your function receives three tensors (Query, Key, and Value in Transformer) as inputs\n",
        "- The input tensors are 3D tensors\n",
        "- The dimension of input tensors are identical (common case in Transformer models); [B, L, D] where B, L, and D are batch, sequence length, and Q/K/V dimension\n",
        "- You do not have to multiply a weight matrix to Q, K and V; assume that we already computed that for Q, K, and V\n",
        "- Assume that we are not masking any rows out."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff1e63f",
      "metadata": {
        "id": "3ff1e63f"
      },
      "source": [
        "### Your Implementation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "8cf4db87",
      "metadata": {
        "id": "8cf4db87"
      },
      "outputs": [],
      "source": [
        "def my_scaled_dot_product_attention(\n",
        "    query: torch.Tensor, # Tensor [B, L, D]\n",
        "    key: torch.Tensor,   # Tensor [B, L, D]\n",
        "    value: torch.Tensor, # Tensor [B, L, D]\n",
        ") -> torch.Tensor:\n",
        "    # We will target the common case: the shape of\n",
        "    # Q, K, and V matrices are the same.\n",
        "    assert(query.shape == key.shape)\n",
        "    assert(value.shape == key.shape)\n",
        "\n",
        "    # Calculate the dot product of the query and key\n",
        "    dot_product = torch.bmm(query, key.transpose(1, 2))\n",
        "\n",
        "    # Scale the dot product by the square root of the dimension of the key\n",
        "    D = key.size(-1)\n",
        "    scaled_dot_product = dot_product / torch.sqrt(torch.tensor(D).float())\n",
        "\n",
        "    # Optional: mask\n",
        "    pass\n",
        "\n",
        "    # Apply the softmax function to the scaled dot product tensor\n",
        "    attention_weights = torch.nn.functional.softmax(scaled_dot_product, dim=-1)\n",
        "\n",
        "    # Multiply the attention weights by the value tensor to get the output\n",
        "    output = torch.bmm(attention_weights, value)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e3d99b9",
      "metadata": {
        "id": "0e3d99b9"
      },
      "source": [
        "### Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "eff1ca45",
      "metadata": {
        "id": "eff1ca45"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Test program: Please do not modify this cell\n",
        "def test_scaled_dot_product_attention(num_tests = number_of_tests):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "\n",
        "    for test_id in range(num_tests):\n",
        "\n",
        "        batch_size = torch.randint(1,32,[1,])\n",
        "        seq_length = torch.randint(32,128,[1,])\n",
        "        dim_size = torch.randint(128,256,[1,])\n",
        "\n",
        "\n",
        "        query = torch.rand(batch_size, seq_length, dim_size, dtype=torch.float)\n",
        "        key = torch.rand(batch_size, seq_length, dim_size, dtype=torch.float)\n",
        "        value = torch.rand(batch_size, seq_length, dim_size, dtype=torch.float)\n",
        "\n",
        "        # Compute the reference output\n",
        "        reference_output = F.scaled_dot_product_attention(query,key,value)\n",
        "        # Compute the test output\n",
        "        test_output = my_scaled_dot_product_attention(query,key,value)\n",
        "\n",
        "        if torch.allclose(reference_output, test_output,  atol=1e-3):\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "        print(\"Finished test run {}\".format(test_id))\n",
        "\n",
        "    return (correct, incorrect, num_tests)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "0ee207c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ee207c7",
        "outputId": "212a9569-8260-45ca-d095-fbea6e4f70ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished test run 0\n",
            "Finished test run 1\n",
            "Finished test run 2\n",
            "Finished test run 3\n",
            "Finished test run 4\n",
            "Finished test run 5\n",
            "Finished test run 6\n",
            "Finished test run 7\n",
            "Finished test run 8\n",
            "Finished test run 9\n",
            "<Test Problem 4 Results>\n",
            "- Correct: 10\n",
            "- Incorrect: 0\n",
            "-> Passed 100.0% of tests\n"
          ]
        }
      ],
      "source": [
        "test_res = test_scaled_dot_product_attention(10)\n",
        "test_res_printer(test_res, \"Problem 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba2bbeb",
      "metadata": {
        "id": "bba2bbeb"
      },
      "source": [
        "## Problem 5 (30 pts)\n",
        "\n",
        "In this problem, we will implement asymmetric linear quantization function. Your function will receive a 2D tensor as an input and quantize the tensor into UINT8 type (unsigned 8 bit integer). Your function needs to (1) investigate the values to identify minimum and maximum values, (2) compute the scale and zero point based on the linear quantization method, and (3) quantize the input tensor using the computed scale and zero point.\n",
        "\n",
        "* Note: This problem is about the asymmetric quantization; you don't need to clip (or clamp) values like the symmetric quantization did."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e0f0993",
      "metadata": {
        "id": "3e0f0993"
      },
      "source": [
        "### Your Implementation (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "d9684e19",
      "metadata": {
        "id": "d9684e19"
      },
      "outputs": [],
      "source": [
        "# Implement this cell\n",
        "\n",
        "def my_uint8_asymmetric_quantization(input_tensor):\n",
        "    # UINT8 asymmetric quantization\n",
        "    min_val, max_val = torch.min(input_tensor), torch.max(input_tensor)\n",
        "\n",
        "    scale = (max_val - min_val) / 255\n",
        "    zero = -min_val / scale\n",
        "\n",
        "    output = torch.round(input_tensor / scale + zero)\n",
        "\n",
        "    output = output.type(torch.uint8)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b6a616",
      "metadata": {
        "id": "83b6a616"
      },
      "source": [
        "### Test Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "3cd69abb",
      "metadata": {
        "id": "3cd69abb"
      },
      "outputs": [],
      "source": [
        "# Test inputs and reference outputs\n",
        "# Please do not modify this cell\n",
        "\n",
        "test_input1 = torch.Tensor(\n",
        "        [[-0.9191,  0.9685,  0.7212, -0.9223],\n",
        "        [ 0.5400, -0.9494,  0.9794,  0.0037],\n",
        "        [ 0.9669, -0.8960, -0.5399,  0.3006],\n",
        "        [-0.9815,  0.9160, -0.2564,  0.2825]]\n",
        ")\n",
        "\n",
        "ref_output1 = torch.Tensor(\n",
        "        [[  8, 254, 222,   8],\n",
        "        [198,   5, 255, 128],\n",
        "        [254,  11,  58, 167],\n",
        "        [  0, 247,  95, 165]]\n",
        ")\n",
        "\n",
        "test_input2 = torch.Tensor(\n",
        "        [[ 0.5587,  0.7415,  0.8307, -0.4854],\n",
        "        [ 0.5472,  0.1830, -0.0901,  0.3482],\n",
        "        [-0.5323, -0.3921,  0.1843,  0.1048],\n",
        "        [-0.7526,  0.0928,  0.3061, -0.2401]]\n",
        ")\n",
        "\n",
        "ref_output2 = torch.Tensor(\n",
        "        [[211, 240, 255,  43],\n",
        "        [209, 150, 106, 177],\n",
        "        [ 35,  58, 151, 138],\n",
        "        [  0, 136, 170,  82]]\n",
        ")\n",
        "\n",
        "test_input3 = torch.Tensor(\n",
        "        [[ 0.9660, -0.4789, -0.7847,  0.1772],\n",
        "        [-0.0440, -0.6890,  0.7417,  0.1999],\n",
        "        [ 0.5212,  0.0673,  0.6502,  0.9912],\n",
        "        [-0.1901,  0.0888,  0.4853,  0.3232]]\n",
        ")\n",
        "\n",
        "ref_output3 = torch.Tensor(\n",
        "        [[252,  44,   0, 138],\n",
        "        [107,  14, 220, 142],\n",
        "        [188, 123, 206, 255],\n",
        "        [ 86, 126, 183, 159]]\n",
        ")\n",
        "\n",
        "\n",
        "test_input4 = torch.Tensor(\n",
        "        [[-0.4374,  0.8469,  0.6904, -0.7379],\n",
        "        [ 0.4810,  0.7268,  0.1416,  0.0273],\n",
        "        [ 0.4733,  0.4300, -0.7830,  0.2241],\n",
        "        [-0.9042,  0.7450, -0.3024,  0.5965]]\n",
        ")\n",
        "\n",
        "ref_output4 = torch.Tensor(\n",
        "        [[ 68, 255, 233,  25],\n",
        "        [202, 238, 153, 136],\n",
        "        [201, 195,  18, 165],\n",
        "        [  0, 240,  88, 219]]\n",
        ")\n",
        "\n",
        "\n",
        "test_input5 = torch.Tensor(\n",
        "        [[ 0.6637, -0.5765, -0.5117,  0.8479],\n",
        "        [ 0.2494,  0.0126, -0.0602,  0.9708],\n",
        "        [ 0.4532, -0.6239, -0.2159, -0.3135],\n",
        "        [ 0.7132,  0.9600, -0.6399, -0.5454]]\n",
        ")\n",
        "\n",
        "ref_output5 = torch.Tensor(\n",
        "        [[206,  10,  20, 235],\n",
        "        [140, 103,  91, 255],\n",
        "        [173,   2,  67,  51],\n",
        "        [214, 253,   0,  15]]\n",
        ")\n",
        "\n",
        "test_inputs = [test_input1, test_input2, test_input3, test_input4, test_input5]\n",
        "ref_outputs = [ref_output1, ref_output2, ref_output3, ref_output4, ref_output5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "87783997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87783997",
        "outputId": "01928a2d-43f8-4260-91ab-f2e12096f511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Test Problem5 Results>\n",
            "- Correct: 5\n",
            "- Incorrect: 0\n",
            "-> Passed 100.0% of tests\n"
          ]
        }
      ],
      "source": [
        "# You can validate your implementation using these tests\n",
        "\n",
        "def test_asymmetric_quantization():\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    num_tests = len(test_inputs)\n",
        "    for test_id in range(num_tests):\n",
        "        test_input = test_inputs[test_id]\n",
        "        ref_output = ref_outputs[test_id]\n",
        "        test_output = my_uint8_asymmetric_quantization(test_input)\n",
        "\n",
        "        # Using the tolerance of 1 as different rounding direction can flip the results\n",
        "        if torch.allclose(ref_output.to(torch.float), test_output.to(torch.float), atol=1):\n",
        "            correct += 1\n",
        "        else:\n",
        "            incorrect += 1\n",
        "\n",
        "    return (correct, incorrect, num_tests)\n",
        "\n",
        "test_res = test_asymmetric_quantization()\n",
        "test_res_printer(test_res, \"Problem5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f75bd6",
      "metadata": {
        "id": "b6f75bd6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}